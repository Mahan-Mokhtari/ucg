# backend/app.py

import os
from flask import Flask, request, jsonify
from faster_whisper import WhisperModel
import subprocess
import wave
import webrtcvad

app = Flask(__name__)

# Upload folder path relative to this script
UPLOAD_FOLDER = os.path.join(os.path.dirname(__file__), "uploads")
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Load Whisper model once at startup
print("🔄 Loading Whisper model (large)...")
model = WhisperModel("large")
print("✅ Whisper model loaded.")

def format_timestamp(seconds):
    """Format seconds as SRT timestamp (HH:MM:SS,mmm)."""
    hrs = int(seconds // 3600)
    mins = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds - int(seconds)) * 1000)
    return f"{hrs:02}:{mins:02}:{secs:02},{millis:03}"

def extract_audio_from_mp4(mp4_path, wav_path):
    """Extract mono 16kHz WAV audio from MP4 using ffmpeg."""
    print(f"🎵 Extracting audio from {mp4_path} to {wav_path}...")
    command = [
        "ffmpeg", "-y", "-i", mp4_path,
        "-ac", "1", "-ar", "16000", wav_path
    ]
    subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    print("✅ Audio extraction complete.")

def read_wave(path):
    """Read WAV file and return PCM audio bytes and sample rate."""
    with wave.open(path, "rb") as wf:
        sample_rate = wf.getframerate()
        pcm_data = wf.readframes(wf.getnframes())
    print(f"🎧 Loaded WAV audio from {path} (Sample rate: {sample_rate} Hz).")
    return pcm_data, sample_rate

def write_wave(path, audio, sample_rate):
    """Write PCM audio bytes to WAV file with given sample rate."""
    with wave.open(path, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)  # 16-bit audio
        wf.setframerate(sample_rate)
        wf.writeframes(audio)
    print(f"💾 Cleaned audio written to {path}.")

def frame_generator(frame_duration_ms, audio, sample_rate):
    """Generate audio frames for VAD from raw PCM bytes."""
    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)  # 2 bytes per sample
    offset = 0
    while offset + n < len(audio):
        yield audio[offset:offset + n]
        offset += n

def vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, vad, frames):
    """
    Use VAD to collect voiced audio segments from frames.
    Yields raw PCM byte chunks of voiced audio.
    """
    num_padding_frames = int(padding_duration_ms / frame_duration_ms)
    ring_buffer = []
    triggered = False
    voiced_frames = []

    for frame in frames:
        is_speech = vad.is_speech(frame, sample_rate)

        if not triggered:
            ring_buffer.append(frame)
            if len(ring_buffer) > num_padding_frames:
                ring_buffer.pop(0)
            if sum(vad.is_speech(f, sample_rate) for f in ring_buffer) > 0.9 * len(ring_buffer):
                triggered = True
                voiced_frames.extend(ring_buffer)
                ring_buffer = []
        else:
            voiced_frames.append(frame)
            ring_buffer.append(frame)
            if len(ring_buffer) > num_padding_frames:
                ring_buffer.pop(0)
            if sum(vad.is_speech(f, sample_rate) for f in ring_buffer) < 0.1 * len(ring_buffer):
                triggered = False
                yield b"".join(voiced_frames)
                ring_buffer = []
                voiced_frames = []

    if voiced_frames:
        yield b"".join(voiced_frames)

@app.route("/transcribe", methods=["POST"])
def transcribe():
    print("📥 Received transcription request.")

    if "file" not in request.files:
        print("⚠️ No file part in request.")
        return jsonify({"error": "No file uploaded"}), 400

    file = request.files["file"]
    if file.filename == "":
        print("⚠️ No file selected.")
        return jsonify({"error": "Empty filename"}), 400

    filename = file.filename
    filepath = os.path.join(UPLOAD_FOLDER, filename)
    print(f"💾 Saving uploaded file to {filepath}...")
    file.save(filepath)
    print("✅ File saved.")

    # Extract audio from MP4
    wav_path = os.path.join(UPLOAD_FOLDER, "temp.wav")
    cleaned_wav_path = os.path.join(UPLOAD_FOLDER, "cleaned.wav")
    extract_audio_from_mp4(filepath, wav_path)

    # Load extracted audio for VAD
    audio, sample_rate = read_wave(wav_path)

    # Run VAD to detect speech frames
    print("🎤 Running Voice Activity Detection (VAD)...")
    vad = webrtcvad.Vad(2)  # Aggressiveness mode 2 (moderate)
    frames = list(frame_generator(30, audio, sample_rate))
    segments = list(vad_collector(sample_rate, 30, 300, vad, frames))
    print(f"✅ VAD complete. {len(segments)} speech segments found.")

    # Combine voiced segments and save cleaned audio
    cleaned_audio = b"".join(segments)
    write_wave(cleaned_wav_path, cleaned_audio, sample_rate)

    # Transcribe cleaned audio using Whisper
    print("🔊 Transcribing cleaned audio with Whisper...")
    segments, _ = model.transcribe(cleaned_wav_path, word_timestamps=True)

    # Prepare SRT subtitles with confidence filtering
    CONFIDENCE_THRESHOLD = 0.7
    srt_output = ""
    index = 1
    for segment in segments:
        words = segment.words
        if not words:
            continue
        avg_confidence = sum(word.probability for word in words) / len(words)
        if avg_confidence < CONFIDENCE_THRESHOLD:
            continue

        start = format_timestamp(segment.start)
        end = format_timestamp(segment.end)
        text = segment.text.strip()

        srt_output += f"{index}\n{start} --> {end}\n{text}\n\n"
        index += 1

    print("✅ Transcription complete. Returning subtitles.")
    return srt_output, 200, {"Content-Type": "text/plain; charset=utf-8"}

if __name__ == "__main__":
    print("🚀 Starting Flask app on http://localhost:5000")
    app.run(debug=True)


///////////////////////////
import subprocess
import wave
import webrtcvad
from faster_whisper import WhisperModel
import pkg_resources

def format_timestamp(seconds):
    hrs = int(seconds // 3600)
    mins = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds - int(seconds)) * 1000)
    return f"{hrs:02}:{mins:02}:{secs:02},{millis:03}"


def extract_audio_from_mp4(mp4_path, wav_path):
    # Extract mono 16kHz wav audio from mp4 with ffmpeg
    command = [
        "ffmpeg", "-y", "-i", mp4_path,
        "-ac", "1", "-ar", "16000", wav_path
    ]
    subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)


def read_wave(path):
    with wave.open(path, "rb") as wf:
        sample_rate = wf.getframerate()
        pcm_data = wf.readframes(wf.getnframes())
    return pcm_data, sample_rate


def write_wave(path, audio, sample_rate):
    with wave.open(path, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)  # 16-bit audio
        wf.setframerate(sample_rate)
        wf.writeframes(audio)


def vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, vad, frames):
    """Generator that yields segments of speech."""
    num_padding_frames = int(padding_duration_ms / frame_duration_ms)
    ring_buffer = []
    triggered = False

    voiced_frames = []

    for frame in frames:
        is_speech = vad.is_speech(frame, sample_rate)

        if not triggered:
            ring_buffer.append(frame)
            if len(ring_buffer) > num_padding_frames:
                ring_buffer.pop(0)
            if sum(vad.is_speech(f, sample_rate) for f in ring_buffer) > 0.9 * len(ring_buffer):
                triggered = True
                voiced_frames.extend(ring_buffer)
                ring_buffer = []
        else:
            voiced_frames.append(frame)
            ring_buffer.append(frame)
            if len(ring_buffer) > num_padding_frames:
                ring_buffer.pop(0)
            if sum(vad.is_speech(f, sample_rate) for f in ring_buffer) < 0.1 * len(ring_buffer):
                triggered = False
                yield b"".join(voiced_frames)
                ring_buffer = []
                voiced_frames = []

    if voiced_frames:
        yield b"".join(voiced_frames)


def frame_generator(frame_duration_ms, audio, sample_rate):
    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)  # 2 bytes per sample (16 bit)
    offset = 0
    while offset + n < len(audio):
        yield audio[offset:offset + n]
        offset += n


# Step 1: Extract audio to WAV
extract_audio_from_mp4("file.mp4", "file.wav")

# Step 2: Load audio for VAD
audio, sample_rate = read_wave("file.wav")

# Step 3: Run VAD
vad = webrtcvad.Vad(2)  # 0-3 aggressiveness (2 is moderate)
frames = list(frame_generator(30, audio, sample_rate))
segments = list(vad_collector(sample_rate, 30, 300, vad, frames))

# Step 4: Combine voiced segments and save cleaned audio
cleaned_audio = b"".join(segments)
write_wave("file_clean.wav", cleaned_audio, sample_rate)

# Step 5: Load Whisper model and transcribe cleaned audio
model = WhisperModel("large")
segments, info = model.transcribe("file_clean.wav", word_timestamps=True)

CONFIDENCE_THRESHOLD = 0.7

with open("output.srt", "w", encoding="utf-8") as srt_file:
    index = 1
    for segment in segments:
        words = segment.words
        if not words:
            continue

        avg_confidence = sum(word.probability for word in words) / len(words)
        if avg_confidence < CONFIDENCE_THRESHOLD:
            continue

        start = format_timestamp(segment.start)
        end = format_timestamp(segment.end)
        text = segment.text.strip()

        srt_file.write(f"{index}\n")
        srt_file.write(f"{start} --> {end}\n")
        srt_file.write(f"{text}\n\n")
        index += 1

print("✅ Subtitles saved as output.srt (non-speech removed with VAD)")
